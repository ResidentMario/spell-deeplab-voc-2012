{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-gpu pytorch (horovod)\n",
    "\n",
    "This time using `Horovod`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ../models/4_pytorch_distributed_horovod.py\n"
    }
   ],
   "source": [
    "%%writefile ../models/4_pytorch_distributed_horovod.py\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import horovod.torch as hvd\n",
    "\n",
    "# VOCSegmentation returns a raw dataset: images are non-resized and in the PIL format. To transform them\n",
    "# to something suitable for input to PyTorch, we need to wrap the output in our own dataset class.\n",
    "class PascalVOCSegmentationDataset(Dataset):\n",
    "    def __init__(self, raw):\n",
    "        super().__init__()\n",
    "        self._dataset = raw\n",
    "        self.resize_img = torchvision.transforms.Resize((256, 256), interpolation=PIL.Image.BILINEAR)\n",
    "        self.resize_segmap = torchvision.transforms.Resize((256, 256), interpolation=PIL.Image.NEAREST)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, segmap = self._dataset[idx]\n",
    "        img, segmap = self.resize_img(img), self.resize_segmap(segmap)\n",
    "        img, segmap = np.array(img), np.array(segmap)\n",
    "        img, segmap = (img / 255).astype('float32'), segmap.astype('int32')\n",
    "        img = np.transpose(img, (-1, 0, 1))\n",
    "\n",
    "        # The PASCAL VOC dataset PyTorch provides labels the edges surrounding classes in 255-valued\n",
    "        # pixels in the segmentation map. However, PyTorch requires class values to be contiguous\n",
    "        # in range 0 through n_classes, so we must relabel these pixels to 21.\n",
    "        segmap[segmap == 255] = 21\n",
    "        \n",
    "        return img, segmap\n",
    "\n",
    "def get_dataloader():\n",
    "    _PascalVOCSegmentationDataset = torchvision.datasets.VOCSegmentation(\n",
    "        '/mnt/pascal_voc_segmentation/', year='2012', image_set='train', download=True,\n",
    "        transform=None, target_transform=None, transforms=None\n",
    "    )\n",
    "    dataset = PascalVOCSegmentationDataset(_PascalVOCSegmentationDataset)\n",
    "    # NEW\n",
    "    # Distributed sampler.\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        dataset, num_replicas=hvd.size(), rank=hvd.rank()\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=8, shuffle=False, sampler=sampler\n",
    "    )\n",
    "    \n",
    "    return dataloader, sampler\n",
    "\n",
    "def get_model():\n",
    "    # num_classes is 22. PASCAL VOC includes 20 classes of interest, 1 background class, and the 1\n",
    "    # special border class mentioned in the previous comment. 20 + 1 + 1 = 22.\n",
    "    DeepLabV3 = torchvision.models.segmentation.deeplabv3_resnet101(\n",
    "        pretrained=False, progress=True, num_classes=22, aux_loss=None\n",
    "    )\n",
    "    model = DeepLabV3\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(NUM_EPOCHS):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        # NEW:\n",
    "        # set epoch to sampler for shuffling.\n",
    "        sampler.set_epoch(epoch)\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i, (batch, segmap) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.cuda()\n",
    "            segmap = segmap.cuda()\n",
    "\n",
    "            output = model(batch)['out']\n",
    "            loss = criterion(output, segmap.type(torch.int64))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss = loss.item()\n",
    "            # if i % 10 == 0:\n",
    "            #     print(\n",
    "            #         f'Finished epoch {epoch}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "            #     )\n",
    "\n",
    "            if hvd.rank() == 0:\n",
    "                writer.add_scalar('training loss', curr_loss)\n",
    "            losses.append(curr_loss)\n",
    "\n",
    "        # print(\n",
    "        #     f'Finished epoch {epoch}. '\n",
    "        #     f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "        # )\n",
    "        if hvd.rank() == 0 and epoch % 5 == 0:\n",
    "            if not os.path.exists('/spell/checkpoints/'):\n",
    "                os.mkdir('/spell/checkpoints/')\n",
    "            torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "    torch.save(model.state_dict(), f'/spell/checkpoints/model_final.pth')\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # NEW:\n",
    "    # Init horovod\n",
    "    hvd.init()\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    writer = SummaryWriter(f'/spell/tensorboards/model_4')\n",
    "\n",
    "    # since the background class doesn't matter nearly as much as the classes of interest to the\n",
    "    # overall task a more selective loss would be more appropriate, however this training script\n",
    "    # is merely a benchmark so we'll just use simple cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # NEW:\n",
    "    # Download the data on only one thread. Have the rest wait until the download finishes.\n",
    "    if hvd.local_rank() == 0:\n",
    "        get_model()\n",
    "        get_dataloader()\n",
    "    hvd.join()\n",
    "    print(f\"Rank {hvd.rank() + 1}/{hvd.size()} process cleared download barrier.\")\n",
    "    \n",
    "    model = get_model()\n",
    "    dataloader, sampler = get_dataloader()\n",
    "    \n",
    "    # NEW:\n",
    "    # Scale learning learning rate by size.\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3 * hvd.size())\n",
    "\n",
    "    # New:\n",
    "    # Broadcast parameters & optimizer state.\n",
    "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n",
    "\n",
    "    # NEW:\n",
    "    # (optional) Free-ish compression (reduces over-the-wire size -> increases speed).\n",
    "    compression = hvd.Compression.fp16\n",
    "    \n",
    "    # NEW:\n",
    "    # Wrap optimizer with DistributedOptimizer.\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer,\n",
    "                                         named_parameters=model.named_parameters(),\n",
    "                                         compression=compression,\n",
    "                                         op=hvd.Average)\n",
    "    train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m✨ Preparing uncommitted changes…\n",
      "\u001b[0mWarning: Permanently added 'git.spell.ml,35.163.108.189' (ECDSA) to the list of known hosts.\n",
      "git@git.spell.ml: Permission denied (publickey).\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n",
      "\u001b[0m\u001b[31mERROR: \u001b[0mPush to Spell remote failed\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!spell run --machine-type V100x4 \\\n",
    "  --tensorboard-dir /spell/tensorboards/model_4/ \\\n",
    "  --distributed 1 \\\n",
    "  \"python models/4_pytorch_distributed_horovod.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spell run --machine-type V100x4 \\\n",
    "  --github-url https://github.com/spellrun/spell-deeplab-voc-2012.git \\\n",
    "  --tensorboard-dir /spell/tensorboards/model_4/ \\\n",
    "  --distributed 1 \\\n",
    "  \"python models/4_pytorch_distributed_horovod.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spell run --machine-type V100x8 \\\n",
    "#   --github-url https://github.com/spellrun/spell-deeplab-voc-2012.git \\\n",
    "#   --tensorboard-dir /spell/tensorboards/model_4/ \\\n",
    "#   --distributed 1 \\\n",
    "#   \"python models/4_pytorch_distributed_horovod.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}