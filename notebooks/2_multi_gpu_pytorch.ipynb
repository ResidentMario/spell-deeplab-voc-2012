{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-gpu pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  7 18:09:23 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that workspaces running on a multi-GPU instance only record metrics for the *first* GPU device in the details graph. We could do a better job with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here's the basic complete distributed example (copied over from the `scratch` workspace):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "def init_process(rank, size, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "\n",
    "def example(rank, world_size):\n",
    "    init_process(rank, world_size)\n",
    "\n",
    "    # TEMP: override rank with 0 because I'm running on a simple T40x1.\n",
    "    # To get the full effect you need to run this code on an INSTANCE_TYPEx2 machine.\n",
    "    rank = 0\n",
    "    \n",
    "    # create local model\n",
    "    model = nn.Linear(10, 10).to(rank)\n",
    "    # construct DDP model\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    # define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(rank))\n",
    "    labels = torch.randn(20, 10).to(rank)\n",
    "    # backward pass\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Finished process {rank}/{world_size}.\")\n",
    "\n",
    "def main():\n",
    "    world_size = 2\n",
    "    mp.spawn(example,\n",
    "        args=(world_size,),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "tmp = nn.Linear(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TmpModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff1 = tmp\n",
    "\n",
    "    def forward(X):\n",
    "        return self.ff1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/2_pytorch_distributed_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/2_pytorch_distributed_model.py\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# NEW\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# NEW\n",
    "def init_process(rank, size, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "\n",
    "# VOCSegmentation returns a raw dataset: images are non-resized and in the PIL format. To transform them\n",
    "# something suitable for input to PyTorch, we need to wrap the output in our own dataset class.\n",
    "class PascalVOCSegmentationDataset(Dataset):\n",
    "    def __init__(self, raw):\n",
    "        super().__init__()\n",
    "        self._dataset = raw\n",
    "        self.resize_img = torchvision.transforms.Resize((256, 256), interpolation=PIL.Image.BILINEAR)\n",
    "        self.resize_segmap = torchvision.transforms.Resize((256, 256), interpolation=PIL.Image.NEAREST)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, segmap = self._dataset[idx]\n",
    "        img, segmap = self.resize_img(img), self.resize_segmap(segmap)\n",
    "        img, segmap = np.array(img), np.array(segmap)\n",
    "        img, segmap = (img / 255).astype('float32'), segmap.astype('int32')\n",
    "        img = np.transpose(img, (-1, 0, 1))\n",
    "\n",
    "        # The PASCAL VOC dataset PyTorch provides labels the edges surrounding classes in 255-valued\n",
    "        # pixels in the segmentation map. However, PyTorch requires class values to be contiguous\n",
    "        # in range 0 through n_classes, so we must relabel these pixels to 21.\n",
    "        segmap[segmap == 255] = 21\n",
    "        \n",
    "        return img, segmap\n",
    "\n",
    "def get_dataloader(rank, world_size):\n",
    "    _PascalVOCSegmentationDataset = torchvision.datasets.VOCSegmentation(\n",
    "        '/mnt/pascal_voc_segmentation/', year='2012', image_set='train', download=True,\n",
    "        transform=None, target_transform=None, transforms=None\n",
    "    )\n",
    "    dataset = PascalVOCSegmentationDataset(_PascalVOCSegmentationDataset)\n",
    "    \n",
    "    # NEW\n",
    "    sampler = DistributedSampler(dataset, rank=rank, num_replicas=world_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False, sampler=sampler)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# num_classes is 22. PASCAL VOC includes 20 classes of interest, 1 background class, and the 1\n",
    "# special border class mentioned in the previous comment. 20 + 1 + 1 = 22.\n",
    "def get_model():\n",
    "    return torchvision.models.segmentation.deeplabv3_resnet101(\n",
    "        pretrained=False, progress=True, num_classes=22, aux_loss=None\n",
    "    )\n",
    "\n",
    "def train(rank, num_epochs, world_size):\n",
    "    # NEW\n",
    "    init_process(rank, world_size)\n",
    "    print(f\"Rank {rank}/{world_size} training process initialized.\\n\")\n",
    "\n",
    "    # NEW\n",
    "    # Since this is a single-instance multi-GPU training script, it's important that only one\n",
    "    # process handle downloading of the data, to:\n",
    "    #\n",
    "    # * Avoid race conditions implicit in having multiple processes attempt to write to the same\n",
    "    #   file simultaneously.\n",
    "    # * Avoid downloading the data in multiple processes simultaneously.\n",
    "    #\n",
    "    # Since the data is cached on disk, we can construct and discard the dataloader and model in\n",
    "    # the master process only to get the data. The other processes are held back by the barrier.\n",
    "    if rank == 0:\n",
    "        get_dataloader(rank, world_size)\n",
    "        get_model()\n",
    "    dist.barrier()\n",
    "    print(f\"Rank {rank}/{world_size} training process passed data download barrier.\\n\")\n",
    "\n",
    "    model = get_model()\n",
    "        \n",
    "    # NEW\n",
    "    model = DistributedDataParallel(model, device_ids=[rank])\n",
    "\n",
    "    model.cuda(rank)\n",
    "    model.train()\n",
    "    \n",
    "    dataloader = get_dataloader(rank, world_size)\n",
    "    \n",
    "    # since the background class doesn't matter nearly as much as the classes of interest to the\n",
    "    # overall task a more selective loss would be more appropriate, however this training script\n",
    "    # is merely a benchmark so we'll just use simple cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # NEW\n",
    "    # Since we are computing the average of several batches at once (an effective batch size of\n",
    "    # world_size * batch_size) we scale the learning rate to match.\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3 * world_size)\n",
    "    \n",
    "    writer = SummaryWriter(f'/spell/tensorboards/model_2')\n",
    "        \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (batch, segmap) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.cuda(rank)\n",
    "            segmap = segmap.cuda(rank)\n",
    "\n",
    "            output = model(batch)['out']\n",
    "            loss = criterion(output, segmap.type(torch.int64))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss = loss.item()\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}, rank {rank}/{world_size}, batch {i}. '\n",
    "                    f'Loss: {curr_loss:.3f}.\\n'\n",
    "                )\n",
    "            if rank == 0:\n",
    "                writer.add_scalar('training loss', curr_loss)\n",
    "            losses.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}, rank {rank}/{world_size}. '\n",
    "            f'Avg Loss: {np.mean(losses)}; Median Loss: {np.min(losses)}.\\n'\n",
    "        )\n",
    "        \n",
    "        if rank == 0:\n",
    "            if not os.path.exists('/spell/checkpoints/'):\n",
    "                os.mkdir('/spell/checkpoints/')\n",
    "            torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "\n",
    "# NEW\n",
    "NUM_EPOCHS = 20\n",
    "WORLD_SIZE = torch.cuda.device_count()\n",
    "def main():\n",
    "    mp.spawn(train,\n",
    "        args=(NUM_EPOCHS, WORLD_SIZE),\n",
    "        nprocs=WORLD_SIZE,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.spawn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3/4 training process initialized.\n",
      "\n",
      "Rank 0/4 training process initialized.\n",
      "\n",
      "Rank 1/4 training process initialized.\n",
      "\n",
      "Rank 2/4 training process initialized.\n",
      "\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tarUsing downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 0. Loss: 3.202.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 0. Loss: 3.182.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 0. Loss: 3.134.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 0. Loss: 3.235.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 10. Loss: 1.991.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 10. Loss: 2.106.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 10. Loss: 1.620.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 10. Loss: 1.957.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 20. Loss: 1.036.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 20. Loss: 1.327.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 20. Loss: 1.524.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 20. Loss: 1.010.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 30. Loss: 1.319.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 30. Loss: 0.860.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 30. Loss: 1.168.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 30. Loss: 1.073.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 40. Loss: 1.413.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 40. Loss: 1.278.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 40. Loss: 1.566.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 40. Loss: 0.988.\n",
      "\n",
      "Finished epoch 1, rank 0/4. Avg Loss: 1.5371137170687965; Median Loss: 0.9232143759727478.\n",
      "Finished epoch 1, rank 1/4. Avg Loss: 1.5187869784624681; Median Loss: 0.7372373342514038.\n",
      "\n",
      "\n",
      "Finished epoch 1, rank 3/4. Avg Loss: 1.4985719089922698; Median Loss: 0.8696364164352417.\n",
      "\n",
      "Finished epoch 1, rank 2/4. Avg Loss: 1.5026965789172961; Median Loss: 0.8880389928817749.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 0. Loss: 0.829.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 0. Loss: 1.016.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 0. Loss: 1.720.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 0. Loss: 1.278.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 10. Loss: 1.031.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 10. Loss: 1.155.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 10. Loss: 1.184.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 10. Loss: 0.925.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 20. Loss: 0.832.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 20. Loss: 1.087.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 20. Loss: 1.299.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 20. Loss: 0.720.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 30. Loss: 1.021.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 30. Loss: 0.733.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 30. Loss: 0.980.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 30. Loss: 0.878.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 40. Loss: 1.232.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 40. Loss: 1.155.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 40. Loss: 1.400.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 40. Loss: 0.837.\n",
      "\n",
      "Finished epoch 2, rank 0/4. Avg Loss: 1.0728938994200334; Median Loss: 0.6762511134147644.\n",
      "\n",
      "Finished epoch 2, rank 1/4. Avg Loss: 1.0440971669943437; Median Loss: 0.6806023120880127.\n",
      "\n",
      "Finished epoch 2, rank 3/4. Avg Loss: 1.0686634480953217; Median Loss: 0.651738166809082.\n",
      "\n",
      "Finished epoch 2, rank 2/4. Avg Loss: 1.1028440737206002; Median Loss: 0.7204523682594299.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 0. Loss: 0.734.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 0. Loss: 0.868.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 0. Loss: 1.479.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 0. Loss: 1.110.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 10. Loss: 1.096.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 10. Loss: 0.856.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 10. Loss: 0.990.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 10. Loss: 0.798.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 20. Loss: 0.698.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 20. Loss: 0.941.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 20. Loss: 1.099.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 20. Loss: 0.649.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 30. Loss: 0.696.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 30. Loss: 0.936.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 30. Loss: 0.823.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 30. Loss: 0.658.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 40. Loss: 1.031.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 40. Loss: 0.948.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 40. Loss: 1.348.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 40. Loss: 0.742.\n",
      "\n",
      "Finished epoch 3, rank 1/4. Avg Loss: 0.9339331064535223; Median Loss: 0.6268016695976257.\n",
      "Finished epoch 3, rank 0/4. Avg Loss: 0.9380997224994327; Median Loss: 0.582360029220581.\n",
      "\n",
      "\n",
      "Finished epoch 3, rank 3/4. Avg Loss: 0.9584759072117184; Median Loss: 0.5946595668792725.\n",
      "\n",
      "Finished epoch 3, rank 2/4. Avg Loss: 0.9730200923007467; Median Loss: 0.6116057634353638.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 0. Loss: 0.698.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 0. Loss: 0.868.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 0. Loss: 1.267.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 0. Loss: 1.078.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 10. Loss: 1.032.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 10. Loss: 0.791.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 10. Loss: 0.899.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 10. Loss: 0.782.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 20. Loss: 0.710.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 20. Loss: 0.843.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 20. Loss: 1.019.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 20. Loss: 0.596.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 30. Loss: 0.708.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 30. Loss: 0.806.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 30. Loss: 1.005.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 30. Loss: 0.628.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 40. Loss: 1.024.\n",
      "Finished epoch 4, rank 0/4, batch 40. Loss: 0.977.\n",
      "\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 40. Loss: 1.287.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 40. Loss: 0.685.\n",
      "\n",
      "Finished epoch 4, rank 0/4. Avg Loss: 0.9038256018058114; Median Loss: 0.5882312655448914.\n",
      "Finished epoch 4, rank 1/4. Avg Loss: 0.8849697488805522; Median Loss: 0.5341734290122986.\n",
      "\n",
      "\n",
      "Finished epoch 4, rank 3/4. Avg Loss: 0.9092361227325771; Median Loss: 0.5009687542915344.\n",
      "\n",
      "Finished epoch 4, rank 2/4. Avg Loss: 0.9352746916853864; Median Loss: 0.5934159755706787.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 0. Loss: 0.729.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 0. Loss: 0.931.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 0. Loss: 0.817.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 0. Loss: 1.338.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 10. Loss: 1.027.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 10. Loss: 0.746.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 10. Loss: 0.827.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 10. Loss: 0.670.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 20. Loss: 0.608.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 20. Loss: 0.801.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 20. Loss: 0.956.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 20. Loss: 0.548.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 30. Loss: 0.865.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 30. Loss: 0.788.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 30. Loss: 0.711.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 30. Loss: 0.544.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 40. Loss: 0.928.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 40. Loss: 1.054.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 40. Loss: 1.089.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 40. Loss: 0.635.\n",
      "\n",
      "Finished epoch 5, rank 1/4. Avg Loss: 0.8360245124153469; Median Loss: 0.5431686043739319.\n",
      "\n",
      "Finished epoch 5, rank 3/4. Avg Loss: 0.8797486735426862; Median Loss: 0.522502601146698.\n",
      "\n",
      "Finished epoch 5, rank 2/4. Avg Loss: 0.8784292508726534; Median Loss: 0.5439735054969788.\n",
      "\n",
      "Finished epoch 5, rank 0/4. Avg Loss: 0.8464650537656702; Median Loss: 0.5294809341430664.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../models/2_pytorch_distributed_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1.pth  model_2.pth  model_3.pth  model_4.pth  model_5.pth\n"
     ]
    }
   ],
   "source": [
    "%ls ../checkpoints/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}