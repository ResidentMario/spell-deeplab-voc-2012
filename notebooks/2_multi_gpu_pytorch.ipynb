{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-gpu pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  7 18:09:23 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4            Off  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   34C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P8     9W /  70W |     11MiB / 15109MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that workspaces running on a multi-GPU instance only record metrics for the *first* GPU device in the details graph. We could do a better job with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here's the basic complete distributed example (copied over from the `scratch` workspace):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "def init_process(rank, size, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "\n",
    "def example(rank, world_size):\n",
    "    init_process(rank, world_size)\n",
    "\n",
    "    # TEMP: override rank with 0 because I'm running on a simple T40x1.\n",
    "    # To get the full effect you need to run this code on an INSTANCE_TYPEx2 machine.\n",
    "    rank = 0\n",
    "    \n",
    "    # create local model\n",
    "    model = nn.Linear(10, 10).to(rank)\n",
    "    # construct DDP model\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    # define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(rank))\n",
    "    labels = torch.randn(20, 10).to(rank)\n",
    "    # backward pass\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Finished process {rank}/{world_size}.\")\n",
    "\n",
    "def main():\n",
    "    world_size = 2\n",
    "    mp.spawn(example,\n",
    "        args=(world_size,),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "tmp = nn.Linear(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TmpModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff1 = tmp\n",
    "\n",
    "    def forward(X):\n",
    "        return self.ff1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/2_pytorch_distributed_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/2_pytorch_distributed_model.py\n",
    "import numpy as np\n",
    "import nlp\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# NEW\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# NEW\n",
    "def init_process(rank, size, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "\n",
    "class IMDBDataset:\n",
    "    def __init__(self, part):\n",
    "        self.dataset = nlp.load_dataset('imdb')['train']\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.dataset[idx]\n",
    "        label = torch.tensor(review['label'])\n",
    "        text = torch.tensor(self.tokenizer.encode(review['text']))\n",
    "        # The default GPT2 token length is 1024. The IMBD text review corpus is pretty long, and\n",
    "        # the GPT2 BPE tokenizer is pretty verbose, so we exceed this character limit in ~3% of\n",
    "        # cases. Since this is simple benchmark we are ignoring this problem (ConstantPad1d\n",
    "        # just clips the last few out words out).\n",
    "        text = nn.ConstantPad1d((1, 1024 - text.shape[0] - 1), 0)(text)\n",
    "        return {'text': text, 'label': label}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows\n",
    "\n",
    "\n",
    "class IMDBSentimentClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gpt2_config = transformers.GPT2Config()\n",
    "        self.gpt2_model = transformers.GPT2Model(self.gpt2_config)\n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(768, 2**6),\n",
    "            nn.Linear(2**6, 2**4),\n",
    "            nn.Linear(2**4, 2),\n",
    "            nn.LogSoftmax(dim=0)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        hidden_states, _ = self.gpt2_model(tokens)\n",
    "        final_hidden_state = hidden_states[:, -1, :]\n",
    "        out = self.head(final_hidden_state)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_dataloader(rank, world_size):\n",
    "    dataset = IMDBDataset('train')    \n",
    "    \n",
    "    # NEW\n",
    "    sampler = DistributedSampler(dataset, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, sampler=sampler)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def get_model():\n",
    "    return IMDBSentimentClassificationModel()\n",
    "\n",
    "def train(rank, num_epochs, world_size):\n",
    "    print(f\"Rank {rank}/{world_size} training process initialized.\\n\")\n",
    "    \n",
    "    model = get_model()\n",
    "    model.cuda(rank)\n",
    "    model.train()\n",
    "    \n",
    "    # NEW\n",
    "    init_process(rank, world_size)\n",
    "    \n",
    "    # NEW\n",
    "    # Since this is a single-instance multi-GPU training script, it's important\n",
    "    # that only one process handle downloading of the data, to avoid race conditions\n",
    "    # implicit in having multiple processes attempt to write to the same file\n",
    "    # simultaneously.\n",
    "    if rank == 0:\n",
    "        nlp.load_dataset('imdb')\n",
    "        transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    dist.barrier()\n",
    "    print(f\"Rank {rank}/{world_size} training process passed data download barrier.\\n\")\n",
    "    \n",
    "    # NEW\n",
    "    model = DistributedDataParallel(model, device_ids=[rank])\n",
    "    \n",
    "    dataloader = get_dataloader(rank, world_size)\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    # NEW\n",
    "    # Since we are computing the average of several batches at once (an effective batch size of\n",
    "    # world_size * batch_size) we scale the learning rate to match.\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3 * world_size)\n",
    "    \n",
    "    writer = SummaryWriter(f'/spell/tensorboards/model_1')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        losses = []\n",
    "\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            tokens, labels = batch['text'], batch['label']\n",
    "            tokens = tokens.cuda(rank)\n",
    "            labels = labels.cuda(rank)\n",
    "\n",
    "            model.zero_grad()\n",
    "            y_pred = model(tokens)\n",
    "            \n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}, rank {rank}/{world_size}, batch {idx}. '\n",
    "                    f'Loss: {loss:.3f}.\\n'\n",
    "                )\n",
    "            if rank == 0:\n",
    "                writer.add_scalar('training loss', loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}, rank {rank}/{world_size}. '\n",
    "            f'Avg Loss: {np.mean(losses)}; Median Loss: {np.min(losses)}.\\n'\n",
    "        )\n",
    "        \n",
    "        if rank == 0:\n",
    "            if not os.path.exists('/spell/checkpoints/'):\n",
    "                os.mkdir('/spell/checkpoints/')\n",
    "            torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "\n",
    "# NEW\n",
    "NUM_EPOCHS = 20\n",
    "WORLD_SIZE = torch.cuda.device_count()\n",
    "def main():\n",
    "    mp.spawn(train,\n",
    "        args=(NUM_EPOCHS, WORLD_SIZE),\n",
    "        nprocs=WORLD_SIZE,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.spawn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3/4 training process initialized.\n",
      "\n",
      "Rank 0/4 training process initialized.\n",
      "\n",
      "Rank 1/4 training process initialized.\n",
      "\n",
      "Rank 2/4 training process initialized.\n",
      "\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tarUsing downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "Using downloaded and verified file: /mnt/pascal_voc_segmentation/VOCtrainval_11-May-2012.tar\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 0. Loss: 3.202.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 0. Loss: 3.182.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 0. Loss: 3.134.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 0. Loss: 3.235.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 10. Loss: 1.991.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 10. Loss: 2.106.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 10. Loss: 1.620.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 10. Loss: 1.957.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 20. Loss: 1.036.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 20. Loss: 1.327.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 20. Loss: 1.524.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 20. Loss: 1.010.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 30. Loss: 1.319.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 30. Loss: 0.860.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 30. Loss: 1.168.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 30. Loss: 1.073.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 40. Loss: 1.413.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 40. Loss: 1.278.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 40. Loss: 1.566.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 40. Loss: 0.988.\n",
      "\n",
      "Finished epoch 1, rank 0/4. Avg Loss: 1.5371137170687965; Median Loss: 0.9232143759727478.\n",
      "Finished epoch 1, rank 1/4. Avg Loss: 1.5187869784624681; Median Loss: 0.7372373342514038.\n",
      "\n",
      "\n",
      "Finished epoch 1, rank 3/4. Avg Loss: 1.4985719089922698; Median Loss: 0.8696364164352417.\n",
      "\n",
      "Finished epoch 1, rank 2/4. Avg Loss: 1.5026965789172961; Median Loss: 0.8880389928817749.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 0. Loss: 0.829.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 0. Loss: 1.016.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 0. Loss: 1.720.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 0. Loss: 1.278.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 10. Loss: 1.031.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 10. Loss: 1.155.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 10. Loss: 1.184.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 10. Loss: 0.925.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 20. Loss: 0.832.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 20. Loss: 1.087.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 20. Loss: 1.299.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 20. Loss: 0.720.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 30. Loss: 1.021.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 30. Loss: 0.733.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 30. Loss: 0.980.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 30. Loss: 0.878.\n",
      "\n",
      "Finished epoch 2, rank 0/4, batch 40. Loss: 1.232.\n",
      "\n",
      "Finished epoch 2, rank 1/4, batch 40. Loss: 1.155.\n",
      "\n",
      "Finished epoch 2, rank 3/4, batch 40. Loss: 1.400.\n",
      "\n",
      "Finished epoch 2, rank 2/4, batch 40. Loss: 0.837.\n",
      "\n",
      "Finished epoch 2, rank 0/4. Avg Loss: 1.0728938994200334; Median Loss: 0.6762511134147644.\n",
      "\n",
      "Finished epoch 2, rank 1/4. Avg Loss: 1.0440971669943437; Median Loss: 0.6806023120880127.\n",
      "\n",
      "Finished epoch 2, rank 3/4. Avg Loss: 1.0686634480953217; Median Loss: 0.651738166809082.\n",
      "\n",
      "Finished epoch 2, rank 2/4. Avg Loss: 1.1028440737206002; Median Loss: 0.7204523682594299.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 0. Loss: 0.734.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 0. Loss: 0.868.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 0. Loss: 1.479.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 0. Loss: 1.110.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 10. Loss: 1.096.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 10. Loss: 0.856.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 10. Loss: 0.990.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 10. Loss: 0.798.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 20. Loss: 0.698.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 20. Loss: 0.941.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 20. Loss: 1.099.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 20. Loss: 0.649.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 30. Loss: 0.696.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 30. Loss: 0.936.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 30. Loss: 0.823.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 30. Loss: 0.658.\n",
      "\n",
      "Finished epoch 3, rank 0/4, batch 40. Loss: 1.031.\n",
      "\n",
      "Finished epoch 3, rank 1/4, batch 40. Loss: 0.948.\n",
      "\n",
      "Finished epoch 3, rank 3/4, batch 40. Loss: 1.348.\n",
      "\n",
      "Finished epoch 3, rank 2/4, batch 40. Loss: 0.742.\n",
      "\n",
      "Finished epoch 3, rank 1/4. Avg Loss: 0.9339331064535223; Median Loss: 0.6268016695976257.\n",
      "Finished epoch 3, rank 0/4. Avg Loss: 0.9380997224994327; Median Loss: 0.582360029220581.\n",
      "\n",
      "\n",
      "Finished epoch 3, rank 3/4. Avg Loss: 0.9584759072117184; Median Loss: 0.5946595668792725.\n",
      "\n",
      "Finished epoch 3, rank 2/4. Avg Loss: 0.9730200923007467; Median Loss: 0.6116057634353638.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 0. Loss: 0.698.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 0. Loss: 0.868.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 0. Loss: 1.267.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 0. Loss: 1.078.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 10. Loss: 1.032.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 10. Loss: 0.791.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 10. Loss: 0.899.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 10. Loss: 0.782.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 20. Loss: 0.710.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 20. Loss: 0.843.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 20. Loss: 1.019.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 20. Loss: 0.596.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 30. Loss: 0.708.\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 30. Loss: 0.806.\n",
      "\n",
      "Finished epoch 4, rank 0/4, batch 30. Loss: 1.005.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 30. Loss: 0.628.\n",
      "\n",
      "Finished epoch 4, rank 1/4, batch 40. Loss: 1.024.\n",
      "Finished epoch 4, rank 0/4, batch 40. Loss: 0.977.\n",
      "\n",
      "\n",
      "Finished epoch 4, rank 3/4, batch 40. Loss: 1.287.\n",
      "\n",
      "Finished epoch 4, rank 2/4, batch 40. Loss: 0.685.\n",
      "\n",
      "Finished epoch 4, rank 0/4. Avg Loss: 0.9038256018058114; Median Loss: 0.5882312655448914.\n",
      "Finished epoch 4, rank 1/4. Avg Loss: 0.8849697488805522; Median Loss: 0.5341734290122986.\n",
      "\n",
      "\n",
      "Finished epoch 4, rank 3/4. Avg Loss: 0.9092361227325771; Median Loss: 0.5009687542915344.\n",
      "\n",
      "Finished epoch 4, rank 2/4. Avg Loss: 0.9352746916853864; Median Loss: 0.5934159755706787.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 0. Loss: 0.729.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 0. Loss: 0.931.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 0. Loss: 0.817.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 0. Loss: 1.338.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 10. Loss: 1.027.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 10. Loss: 0.746.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 10. Loss: 0.827.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 10. Loss: 0.670.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 20. Loss: 0.608.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 20. Loss: 0.801.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 20. Loss: 0.956.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 20. Loss: 0.548.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 30. Loss: 0.865.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 30. Loss: 0.788.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 30. Loss: 0.711.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 30. Loss: 0.544.\n",
      "\n",
      "Finished epoch 5, rank 0/4, batch 40. Loss: 0.928.\n",
      "\n",
      "Finished epoch 5, rank 1/4, batch 40. Loss: 1.054.\n",
      "\n",
      "Finished epoch 5, rank 3/4, batch 40. Loss: 1.089.\n",
      "\n",
      "Finished epoch 5, rank 2/4, batch 40. Loss: 0.635.\n",
      "\n",
      "Finished epoch 5, rank 1/4. Avg Loss: 0.8360245124153469; Median Loss: 0.5431686043739319.\n",
      "\n",
      "Finished epoch 5, rank 3/4. Avg Loss: 0.8797486735426862; Median Loss: 0.522502601146698.\n",
      "\n",
      "Finished epoch 5, rank 2/4. Avg Loss: 0.8784292508726534; Median Loss: 0.5439735054969788.\n",
      "\n",
      "Finished epoch 5, rank 0/4. Avg Loss: 0.8464650537656702; Median Loss: 0.5294809341430664.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ../models/2_pytorch_distributed_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1.pth  model_2.pth  model_3.pth  model_4.pth  model_5.pth\n"
     ]
    }
   ],
   "source": [
    "%ls ../checkpoints/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}